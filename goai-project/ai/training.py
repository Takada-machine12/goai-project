# ai/training.py
import torch
import numpy as np
import random
from collections import deque
import pickle
import os
from tqdm import tqdm
from copy import deepcopy

from .network import ImprovedGoNeuralNetwork, NetworkTrainer
from .mcts import MCTSPlayer
from go_engine.game import Game

class SelfPlayTrainingSystem:
    """è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, board_size=9, network_config=None, training_config=None):
        """
        åˆæœŸåŒ–
        
        Args:
            board_size: ç›¤é¢ã‚µã‚¤ã‚º
            network_config: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
            training_config: è¨“ç·´è¨­å®š
        """
        self.board_size = board_size
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        if network_config is None:
            network_config = {
                'num_channels': 256,
                'num_residual_blocks': 10,
                'lr': 0.001,
                'weight_decay': 1e-4
            }
        
        if training_config is None:
            training_config = {
                'num_iterations': 100,
                'num_self_play_games': 25,
                'num_mcts_simulations': 400,
                'num_training_epochs': 10,
                'batch_size': 32,
                'memory_size': 100000,
                'c_puct': 1.0,
                'temperature_threshold': 30,
                'model_save_interval': 10
            }
        
        self.network_config = network_config
        self.training_config = training_config
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
        self.network = ImprovedGoNeuralNetwork(
            board_size=board_size,
            num_channels=network_config['num_channels'],
            num_residual_blocks=network_config['num_residual_blocks']
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
        self.trainer = NetworkTrainer(
            self.network,
            lr=network_config['lr'],
            weight_decay=network_config['weight_decay']
        )
        
        # çµŒé¨“ãƒ¡ãƒ¢ãƒªï¼ˆãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ï¼‰
        self.memory = deque(maxlen=training_config['memory_size'])
        
        # çµ±è¨ˆæƒ…å ±
        self.iteration_stats = []
        
    def train(self, save_dir='trained_models'):
        """
        ãƒ¡ã‚¤ãƒ³è¨“ç·´ãƒ«ãƒ¼ãƒ—
        
        Args:
            save_dir: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        os.makedirs(save_dir, exist_ok=True)
        
        print("ğŸ® å›²ç¢AIè‡ªå·±å¯¾æˆ¦å­¦ç¿’é–‹å§‹ï¼")
        print(f"ç›¤é¢ã‚µã‚¤ã‚º: {self.board_size}x{self.board_size}")
        print(f"è¨“ç·´åå¾©å›æ•°: {self.training_config['num_iterations']}")
        print(f"è‡ªå·±å¯¾æˆ¦ã‚²ãƒ¼ãƒ æ•°/åå¾©: {self.training_config['num_self_play_games']}")
        print(f"MCTS ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {self.training_config['num_mcts_simulations']}")
        
        for iteration in range(1, self.training_config['num_iterations'] + 1):
            print(f"\n=== åå¾© {iteration}/{self.training_config['num_iterations']} ===")
            
            # 1. è‡ªå·±å¯¾æˆ¦ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            print("ğŸ¯ è‡ªå·±å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
            self_play_data = self.generate_self_play_data()
            
            # 2. ãƒ¡ãƒ¢ãƒªã«è¿½åŠ 
            self.memory.extend(self_play_data)
            print(f"ğŸ“Š ç·ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {len(self.memory)}")
            
            # 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´
            if len(self.memory) >= self.training_config['batch_size']:
                print("ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´ä¸­...")
                training_stats = self.train_network()
                
                # çµ±è¨ˆã‚’è¨˜éŒ²
                stats = {
                    'iteration': iteration,
                    'memory_size': len(self.memory),
                    'games_played': len(self_play_data),
                    **training_stats
                }
                self.iteration_stats.append(stats)
                
                print(f"ğŸ“ˆ è¨“ç·´æå¤±: {training_stats['avg_total_loss']:.4f}")
                print(f"ğŸ“ˆ ä¾¡å€¤æå¤±: {training_stats['avg_value_loss']:.4f}")
                print(f"ğŸ“ˆ æ–¹ç­–æå¤±: {training_stats['avg_policy_loss']:.4f}")
            
            # 4. å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            if iteration % self.training_config['model_save_interval'] == 0:
                model_path = os.path.join(save_dir, f'model_iteration_{iteration}.pt')
                self.trainer.save_model(model_path)
                print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
                
                # çµ±è¨ˆã‚‚ä¿å­˜
                stats_path = os.path.join(save_dir, f'stats_iteration_{iteration}.pkl')
                with open(stats_path, 'wb') as f:
                    pickle.dump(self.iteration_stats, f)
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        final_model_path = os.path.join(save_dir, 'final_model.pt')
        self.trainer.save_model(final_model_path)
        print(f"âœ… è¨“ç·´å®Œäº†ï¼æœ€çµ‚ãƒ¢ãƒ‡ãƒ«: {final_model_path}")
    
    def generate_self_play_data(self):
        """
        è‡ªå·±å¯¾æˆ¦ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Returns:
            è‡ªå·±å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        all_data = []
        
        # MCTSãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½œæˆ
        mcts_player = MCTSPlayer(
            neural_network=self.network,
            num_simulations=self.training_config['num_mcts_simulations'],
            c_puct=self.training_config['c_puct']
        )
        
        for game_idx in tqdm(range(self.training_config['num_self_play_games']), 
                           desc="è‡ªå·±å¯¾æˆ¦"):
            game_data = self.play_single_game(mcts_player)
            all_data.extend(game_data)
        
        return all_data
    
    def play_single_game(self, mcts_player):
        """
        1å›ã®è‡ªå·±å¯¾æˆ¦ã‚²ãƒ¼ãƒ ã‚’å®Ÿè¡Œ
        
        Args:
            mcts_player: MCTSãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼
            
        Returns:
            ã‚²ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿
        """
        game = Game(self.board_size)
        game_data = []
        
        move_count = 0
        max_moves = self.board_size * self.board_size * 2  # æœ€å¤§æ‰‹æ•°åˆ¶é™
        
        while not game.game_over and move_count < max_moves:
            # ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜
            current_state = deepcopy(game)
            
            # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåºç›¤ã¯é«˜ãã€çµ‚ç›¤ã¯ä½ãï¼‰
            temperature = 1.0 if move_count < self.training_config['temperature_threshold'] else 0.1
            
            # MCTSã§è¡Œå‹•ç¢ºç‡ã‚’å–å¾—
            action_probs = mcts_player.get_action_probs(game, temperature=temperature)
            
            # è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            action_idx = np.random.choice(len(action_probs), p=action_probs)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ‰‹ã«å¤‰æ›
            if action_idx == self.board_size * self.board_size:
                move = None  # ãƒ‘ã‚¹
            else:
                x = action_idx // self.board_size
                y = action_idx % self.board_size
                move = (x, y)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆçŠ¶æ…‹ã€è¡Œå‹•ç¢ºç‡ã€ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰
            game_data.append({
                'state': current_state,
                'action_probs': action_probs,
                'current_player': game.current_player
            })
            
            # æ‰‹ã‚’å®Ÿè¡Œ
            success = game.make_move(move)
            if not success:
                # ä¸æ­£ãªæ‰‹ã®å ´åˆã€ã‚²ãƒ¼ãƒ ã‚’çµ‚äº†
                break
                
            move_count += 1
        
        # ã‚²ãƒ¼ãƒ çµæœã‚’æ±ºå®š
        winner = self.determine_winner(game)
        
        # å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«çµæœã‚’è¿½åŠ 
        for data in game_data:
            # å‹è€…ã‚’ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¦–ç‚¹ã‹ã‚‰è©•ä¾¡
            if winner == 0:  # å¼•ãåˆ†ã‘
                data['value'] = 0.0
            elif winner == data['current_player']:
                data['value'] = 1.0  # å‹ã¡
            else:
                data['value'] = -1.0  # è² ã‘
        
        return game_data
    
    def determine_winner(self, game):
        """
        ã‚²ãƒ¼ãƒ ã®å‹è€…ã‚’æ±ºå®šï¼ˆç°¡å˜ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
        
        Args:
            game: ã‚²ãƒ¼ãƒ çŠ¶æ…‹
            
        Returns:
            å‹è€… (1: é»’, -1: ç™½, 0: å¼•ãåˆ†ã‘)
        """
        board = game.board.board
        black_stones = np.sum(board == 1)
        white_stones = np.sum(board == -1)
        
        # ç°¡å˜ãªåœ°ã®è¨ˆç®—ï¼ˆç©ºç‚¹ã‚’æœ€ã‚‚è¿‘ã„è‰²ã«å‰²ã‚Šå½“ã¦ï¼‰
        black_territory, white_territory = self.count_territory(game)
        
        black_score = black_stones + black_territory
        white_score = white_stones + white_territory + 6.5  # ã‚³ãƒŸ
        
        if black_score > white_score:
            return 1  # é»’ã®å‹ã¡
        elif white_score > black_score:
            return -1  # ç™½ã®å‹ã¡
        else:
            return 0  # å¼•ãåˆ†ã‘
    
    def count_territory(self, game):
        """
        ç°¡å˜ãªåœ°ã®è¨ˆç®—
        
        Args:
            game: ã‚²ãƒ¼ãƒ çŠ¶æ…‹
            
        Returns:
            (é»’ã®åœ°, ç™½ã®åœ°)
        """
        board = game.board.board
        visited = np.zeros_like(board, dtype=bool)
        black_territory = 0
        white_territory = 0
        
        for i in range(self.board_size):
            for j in range(self.board_size):
                if board[i, j] == 0 and not visited[i, j]:
                    # ç©ºç‚¹ã‹ã‚‰å§‹ã¾ã‚‹é ˜åŸŸã‚’æ¢ç´¢
                    territory, owner = self.flood_fill_territory(board, i, j, visited)
                    
                    if owner == 1:  # é»’ã®åœ°
                        black_territory += territory
                    elif owner == -1:  # ç™½ã®åœ°
                        white_territory += territory
        
        return black_territory, white_territory
    
    def flood_fill_territory(self, board, start_x, start_y, visited):
        """
        Flood fillã§åœ°ã‚’è¨ˆç®—
        
        Args:
            board: ç›¤é¢
            start_x, start_y: é–‹å§‹ä½ç½®
            visited: è¨ªå•æ¸ˆã¿ãƒã‚¹ã‚¯
            
        Returns:
            (åœ°ã®å¤§ãã•, æ‰€æœ‰è€…)
        """
        if board[start_x, start_y] != 0 or visited[start_x, start_y]:
            return 0, 0
        
        territory_points = []
        stack = [(start_x, start_y)]
        neighboring_colors = set()
        
        while stack:
            x, y = stack.pop()
            if visited[x, y]:
                continue
                
            visited[x, y] = True
            territory_points.append((x, y))
            
            # éš£æ¥ã™ã‚‹ç‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:
                nx, ny = x + dx, y + dy
                if 0 <= nx < self.board_size and 0 <= ny < self.board_size:
                    if board[nx, ny] == 0 and not visited[nx, ny]:
                        stack.append((nx, ny))
                    elif board[nx, ny] != 0:
                        neighboring_colors.add(board[nx, ny])
        
        # åœ°ã®æ‰€æœ‰è€…ã‚’æ±ºå®š
        if len(neighboring_colors) == 1:
            owner = next(iter(neighboring_colors))
            return len(territory_points), owner
        else:
            return 0, 0  # ä¸­ç«‹åœ°
    
    def train_network(self):
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´
        
        Returns:
            è¨“ç·´çµ±è¨ˆ
        """
        total_losses = []
        value_losses = []
        policy_losses = []
        
        # ãƒ¡ãƒ¢ãƒªã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch_size = self.training_config['batch_size']
        epochs = self.training_config['num_training_epochs']
        
        for epoch in range(epochs):
            # ãƒãƒƒãƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            batch_indices = random.sample(range(len(self.memory)), 
                                        min(batch_size, len(self.memory)))
            batch_data = [self.memory[i] for i in batch_indices]
            
            # ãƒãƒƒãƒã‚’æº–å‚™
            batch_states = []
            batch_action_probs = []
            batch_values = []
            
            for data in batch_data:
                # çŠ¶æ…‹ã‚’ç‰¹å¾´é‡ã«å¤‰æ›
                features = self.network._game_state_to_features(data['state'])
                batch_states.append(features.squeeze(0))
                batch_action_probs.append(data['action_probs'])
                batch_values.append(data['value'])
            
            # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            batch_states = torch.stack(batch_states)
            batch_action_probs = torch.FloatTensor(batch_action_probs)
            batch_values = torch.FloatTensor(batch_values)
            
            # è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—
            total_loss, value_loss, policy_loss = self.trainer.train_step(
                batch_states, batch_action_probs, batch_values
            )
            
            total_losses.append(total_loss)
            value_losses.append(value_loss)
            policy_losses.append(policy_loss)
        
        # å­¦ç¿’ç‡ã‚’æ›´æ–°
        self.trainer.scheduler.step()
        
        return {
            'avg_total_loss': np.mean(total_losses),
            'avg_value_loss': np.mean(value_losses),
            'avg_policy_loss': np.mean(policy_losses)
        }
    
    def load_model(self, model_path):
        """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        self.trainer.load_model(model_path)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")
    
    def evaluate_against_random(self, num_games=10):
        """
        ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã®å¯¾æˆ¦ã§è©•ä¾¡
        
        Args:
            num_games: å¯¾æˆ¦ã‚²ãƒ¼ãƒ æ•°
            
        Returns:
            å‹ç‡
        """
        mcts_player = MCTSPlayer(
            neural_network=self.network,
            num_simulations=200,  # è©•ä¾¡æ™‚ã¯å°‘ã—è»½ã
            c_puct=self.training_config['c_puct']
        )
        
        wins = 0
        
        for game_idx in tqdm(range(num_games), desc="è©•ä¾¡å¯¾æˆ¦"):
            game = Game(self.board_size)
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã«å…ˆæ‰‹å¾Œæ‰‹ã‚’æ±ºå®š
            ai_is_black = random.choice([True, False])
            
            move_count = 0
            max_moves = self.board_size * self.board_size * 2
            
            while not game.game_over and move_count < max_moves:
                if (game.current_player == 1 and ai_is_black) or \
                   (game.current_player == -1 and not ai_is_black):
                    # AIã®æ‰‹ç•ª
                    move = mcts_player.get_move(game)
                else:
                    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰‹ç•ª
                    legal_moves = game.get_legal_moves()
                    move = random.choice(legal_moves) if legal_moves else None
                
                if not game.make_move(move):
                    break
                    
                move_count += 1
            
            # å‹è€…åˆ¤å®š
            winner = self.determine_winner(game)
            
            if (winner == 1 and ai_is_black) or (winner == -1 and not ai_is_black):
                wins += 1
        
        win_rate = wins / num_games
        print(f"ğŸ† å¯¾ãƒ©ãƒ³ãƒ€ãƒ å‹ç‡: {win_rate:.2%} ({wins}/{num_games})")
        return win_rate


# ä½¿ç”¨ä¾‹
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # è¨“ç·´ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
    training_system = SelfPlayTrainingSystem(
        board_size=9,
        network_config={
            'num_channels': 128,  # è»½é‡åŒ–
            'num_residual_blocks': 6,  # è»½é‡åŒ–
            'lr': 0.002,
            'weight_decay': 1e-4
        },
        training_config={
            'num_iterations': 50,  # å°‘ãªã‚ã§ãƒ†ã‚¹ãƒˆ
            'num_self_play_games': 10,  # å°‘ãªã‚ã§ãƒ†ã‚¹ãƒˆ
            'num_mcts_simulations': 200,  # è»½é‡åŒ–
            'num_training_epochs': 5,
            'batch_size': 16,
            'memory_size': 10000,
            'c_puct': 1.0,
            'temperature_threshold': 15,
            'model_save_interval': 10
        }
    )
    
    # è¨“ç·´å®Ÿè¡Œ
    training_system.train()
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã®å¯¾æˆ¦ã§è©•ä¾¡
    training_system.evaluate_against_random(num_games=5)


if __name__ == "__main__":
    main()